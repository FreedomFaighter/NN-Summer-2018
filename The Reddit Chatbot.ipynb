{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Deep-Learning Reddit Chatbot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is reddit?\n",
    "\n",
    "[Inspired by pythonprogramming.net](https://pythonprogramming.net/bidirectional-attention-mechanism-chatbot-deep-learning-python-tensorflow/?completed=/training-model-chatbot-deep-learning-python-tensorflow/) and the [neuro-machine translator](https://github.com/tensorflow/nmt)\n",
    "\n",
    "\n",
    "[Reddit](https://www.reddit.com/) is a massive forum on the internet, and it is famous for having many diverse user made \"subreddits\".\n",
    "\n",
    "\n",
    "[A famous reddit post](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/?st=j9udbxta&sh=69e4fee7) has publically made avaliable 1.7 billion reddit comments compressed as 250GBs of data, which is nice considering Reddit API under PRAW and scraping data individually is not worth the hassle.\n",
    "\n",
    "\n",
    "Another user was kind enough to [sort](https://www.reddit.com/r/bigquery/comments/3cej2b/17_billion_reddit_comments_loaded_on_bigquery/?st=jae26i99&sh=3d53e174) them on Google Big Query. This is cost prohibitive at the moment, so I might have to use Shamu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  *The way that Reddit stores their comments is withing a parent-child format, where the parent is the original comment within a comment.\n",
    "\n",
    "  *Another unique feature is the voting feature, where comments are voted.\n",
    "  \n",
    "  *Certain subreddits may hold unique culture and conversational styles that may be useful for a chat-bot provided by real people. We may be able to use the voting system as some sort of a filter comments to provide discrimination between conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We're going to download a sample data of 1 month (no need to download everything, but it would be interesting to toy around with it in Shamu. Ask Dr. Richardson for help)\n",
    "\n",
    "2. Make a database to BUFFER the data. It's SO big that we can't just read it into our puny 32GB of RAM for our training set. Even just a month is big data (Reddit is massive). For SQLite3, we prepare a lot of pre-defined functions that insert themselves as SQL commands on a big database. (One provided by python-programing below)** Help! Dr. Richardson!\n",
    "\n",
    "3. We're going to train our data using a Deep belief net using the theory on [neuro-machine translations](https://github.com/tensorflow/nmt) using something called \"attention mechanisms\" something related to [Long-Short Term Memory networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).  ** Help! Dr. Richardson!\n",
    "\n",
    "LSTMs are can remember decently sequences of tokens up to 10-20 in length fairly well. After, this point, their performance drops. For some reason a \"Bidirectional\" recurrent neural network does pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build a database that's going to store our parent comments with replies. The reason why is because these files are too big for us to just like read into RAM and then create the training files from a month basis (Reddit has many users). Thus,\n",
    "\n",
    "But chances are you're gonna want to eventually if you wanted to create a really\n",
    "\n",
    "Nice chat bot you're gonna be wanting to work on many months of data\n",
    "\n",
    "so maybe possibly billions of comments you do have that your disposal so when that's the case we\n",
    "\n",
    "Probably want to have some sort of database now for the purposes here. Just to keep things simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "timeframe = '2017-03'\n",
    "sql_transaction = []\n",
    "start_row = 0\n",
    "cleanup = 1000000\n",
    "\n",
    "connection = sqlite3.connect('{}.db'.format(timeframe))\n",
    "c = connection.cursor()\n",
    "\n",
    "def create_table():\n",
    "    c.execute(\"CREATE TABLE IF NOT EXISTS parent_reply(parent_id TEXT PRIMARY KEY, comment_id TEXT UNIQUE, parent TEXT, comment TEXT, subreddit TEXT, unix INT, score INT)\")\n",
    "\n",
    "def format_data(data):\n",
    "    data = data.replace('\\n',' newlinechar ').replace('\\r',' newlinechar ').replace('\"',\"'\")\n",
    "    return data\n",
    "\n",
    "def transaction_bldr(sql):\n",
    "    global sql_transaction\n",
    "    sql_transaction.append(sql)\n",
    "    if len(sql_transaction) > 1000:\n",
    "        c.execute('BEGIN TRANSACTION')\n",
    "        for s in sql_transaction:\n",
    "            try:\n",
    "                c.execute(s)\n",
    "            except:\n",
    "                pass\n",
    "        connection.commit()\n",
    "        sql_transaction = []\n",
    "\n",
    "def sql_insert_replace_comment(commentid,parentid,parent,comment,subreddit,time,score):\n",
    "    try:\n",
    "        sql = \"\"\"UPDATE parent_reply SET parent_id = ?, comment_id = ?, parent = ?, comment = ?, subreddit = ?, unix = ?, score = ? WHERE parent_id =?;\"\"\".format(parentid, commentid, parent, comment, subreddit, int(time), score, parentid)\n",
    "        transaction_bldr(sql)\n",
    "    except Exception as e:\n",
    "        print('s0 insertion',str(e))\n",
    "\n",
    "def sql_insert_has_parent(commentid,parentid,parent,comment,subreddit,time,score):\n",
    "    try:\n",
    "        sql = \"\"\"INSERT INTO parent_reply (parent_id, comment_id, parent, comment, subreddit, unix, score) VALUES (\"{}\",\"{}\",\"{}\",\"{}\",\"{}\",{},{});\"\"\".format(parentid, commentid, parent, comment, subreddit, int(time), score)\n",
    "        transaction_bldr(sql)\n",
    "    except Exception as e:\n",
    "        print('s0 insertion',str(e))\n",
    "\n",
    "def sql_insert_no_parent(commentid,parentid,comment,subreddit,time,score):\n",
    "    try:\n",
    "        sql = \"\"\"INSERT INTO parent_reply (parent_id, comment_id, comment, subreddit, unix, score) VALUES (\"{}\",\"{}\",\"{}\",\"{}\",{},{});\"\"\".format(parentid, commentid, comment, subreddit, int(time), score)\n",
    "        transaction_bldr(sql)\n",
    "    except Exception as e:\n",
    "        print('s0 insertion',str(e))\n",
    "\n",
    "def acceptable(data):\n",
    "    if len(data.split(' ')) > 1000 or len(data) < 1:\n",
    "        return False\n",
    "    elif len(data) > 32000:\n",
    "        return False\n",
    "    elif data == '[deleted]':\n",
    "        return False\n",
    "    elif data == '[removed]':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def find_parent(pid):\n",
    "    try:\n",
    "        sql = \"SELECT comment FROM parent_reply WHERE comment_id = '{}' LIMIT 1\".format(pid)\n",
    "        c.execute(sql)\n",
    "        result = c.fetchone()\n",
    "        if result != None:\n",
    "            return result[0]\n",
    "        else: return False\n",
    "    except Exception as e:\n",
    "        #print(str(e))\n",
    "        return False\n",
    "\n",
    "def find_existing_score(pid):\n",
    "    try:\n",
    "        sql = \"SELECT score FROM parent_reply WHERE parent_id = '{}' LIMIT 1\".format(pid)\n",
    "        c.execute(sql)\n",
    "        result = c.fetchone()\n",
    "        if result != None:\n",
    "            return result[0]\n",
    "        else: return False\n",
    "    except Exception as e:\n",
    "        #print(str(e))\n",
    "        return False\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    create_table()\n",
    "    row_counter = 0\n",
    "    paired_rows = 0\n",
    "\n",
    "    #with open('J:/chatdata/reddit_data/{}/RC_{}'.format(timeframe.split('-')[0],timeframe), buffering=1000) as f:\n",
    "    with open('/home/paperspace/reddit_comment_dumps/RC_{}'.format(timeframe), buffering=1000) as f:\n",
    "        for row in f:\n",
    "            #print(row)\n",
    "            #time.sleep(555)\n",
    "            row_counter += 1\n",
    "\n",
    "            if row_counter > start_row:\n",
    "                try:\n",
    "                    row = json.loads(row)\n",
    "                    parent_id = row['parent_id'].split('_')[1]\n",
    "                    body = format_data(row['body'])\n",
    "                    created_utc = row['created_utc']\n",
    "                    score = row['score']\n",
    "                    \n",
    "                    comment_id = row['id']\n",
    "                    \n",
    "                    subreddit = row['subreddit']\n",
    "                    parent_data = find_parent(parent_id)\n",
    "                    \n",
    "                    existing_comment_score = find_existing_score(parent_id)\n",
    "                    if existing_comment_score:\n",
    "                        if score > existing_comment_score:\n",
    "                            if acceptable(body):\n",
    "                                sql_insert_replace_comment(comment_id,parent_id,parent_data,body,subreddit,created_utc,score)\n",
    "                                \n",
    "                    else:\n",
    "                        if acceptable(body):\n",
    "                            if parent_data:\n",
    "                                if score >= 2:\n",
    "                                    sql_insert_has_parent(comment_id,parent_id,parent_data,body,subreddit,created_utc,score)\n",
    "                                    paired_rows += 1\n",
    "                            else:\n",
    "                                sql_insert_no_parent(comment_id,parent_id,body,subreddit,created_utc,score)\n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                            \n",
    "            if row_counter % 100000 == 0:\n",
    "                print('Total Rows Read: {}, Paired Rows: {}, Time: {}'.format(row_counter, paired_rows, str(datetime.now())))\n",
    "\n",
    "            if row_counter > start_row:\n",
    "                if row_counter % cleanup == 0:\n",
    "                    print(\"Cleanin up!\")\n",
    "                    sql = \"DELETE FROM parent_reply WHERE parent IS NULL\"\n",
    "                    c.execute(sql)\n",
    "                    connection.commit()\n",
    "                    c.execute(\"VACUUM\")\n",
    "                    connection.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
