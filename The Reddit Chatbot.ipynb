{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1.0\n",
    "0. Resources Used\n",
    "    0.1 Demonstration\n",
    "    0.2 Mathematics\n",
    "1. What is reddit?\n",
    "    1.1 Why reddit for a chatbot?\n",
    "2. The Dataset\n",
    "\n",
    "Notes to self:\n",
    "\n",
    "Make a database to BUFFER the data. It's SO big that we can't just read it into our puny 32GB of RAM for our training set. Even just a month is big data (Reddit is massive). For SQLite3, we prepare a lot of pre-defined functions that insert themselves as SQL commands on a big database. (One provided by python-programing below)** Help! Dr. Richardson!\n",
    "\n",
    "We're going to train our data using a Deep belief net using the theory on [neuro-machine translations](https://github.com/tensorflow/nmt) using something called \"attention mechanisms\" something related to [Long-Short Term Memory networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).  ** Help! Dr. Richardson!\n",
    "\n",
    "LSTMs are can remember decently sequences of tokens up to 10-20 in length fairly well. After, this point, their performance drops. For some reason a \"Bidirectional\" recurrent neural network does pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Resources Used\n",
    "\n",
    "  *[SQLite3](https://docs.python.org/2/library/sqlite3.html#)\n",
    "  *[Pythonprogramming.net Reddit Chatbot](https://pythonprogramming.net/bidirectional-attention-mechanism-chatbot-deep-learning-python-tensorflow/?completed=/training-model-chatbot-deep-learning-python-tensorflow/) \n",
    "  *[SQL](https://pythonprogramming.net/mysql-intro/)\n",
    "  *[Neuro-Machine Translator](https://github.com/tensorflow/nmt)\n",
    "  *[open function](https://docs.python.org/2/library/functions.html#open)\n",
    "  \n",
    "### About SQLite3 \n",
    "      SQLite is a C library that provides a lightweight disk-based database that doesn’t require a separate server process and allows accessing the database using a nonstandard variant of the SQL query language. Some applications can use SQLite for internal data storage. It’s also possible to prototype an application using SQLite and then port the code to a larger database such as PostgreSQL or Oracle.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 The Deep-Learning Reddit Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#insert finished code here. Make it collapsable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 What is reddit?\n",
    "\n",
    "[Reddit](https://www.redditinc.com/) is a massive social-medial platforms on the internet, and it is famous for basically being the 'forum of forums'. Reddit is fractured into diverse user-made subreddits, which is a sub-forum for any niche topic). \n",
    "\n",
    "What makes reddit specifically good for a chatbot are a few reasons.\n",
    "\n",
    "## Why Reddit for a chatbot?\n",
    "\n",
    "    1) Reddit structures original comments and responses in a tree-like format, allowing it to store massive amounts of human-generated responses to human-generated statement and questions.\n",
    "\n",
    "    2) Reddit uses 2 metrics to denote a somewhat \"general\" sense of value in a comment: voting and gold. Voting is when another user decides to evaluate your comment as positive {+1} or negative {-1}. Gold is when another person has purchased real money to show that your comment is important for some reason. Keep in mind that certain subreddits have different connotations as to what these mean.\n",
    "\n",
    "    3) Because reddit is such a wealth of statement-> responses, we should be able to use a Recurrent Neural Networks model to look for the hidden context between sentences in a statement -> response format. (hidden markov fields to make an \"intelligent\" chatbot.)\n",
    "\n",
    "\n",
    "### Interesting note\n",
    "Certain subreddits may hold a unique micro-culture and conversational styles that may be useful for our chat-bot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A famous reddit post](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/?st=j9udbxta&sh=69e4fee7) has publically made avaliable 1.7 billion reddit comments compressed as 250GBs of data, which is nice considering Reddit API under PRAW and scraping data individually is not worth the hassle.\n",
    "\n",
    "\n",
    "Another user was kind enough to [sort](https://www.reddit.com/r/bigquery/comments/3cej2b/17_billion_reddit_comments_loaded_on_bigquery/?st=jae26i99&sh=3d53e174) them on Google Big Query. This is cost prohibitive at the moment, so I might have to use Shamu. \n",
    "\n",
    "For the purposes of this of project, we shall only use 1 month's worth of data as a sample, and create a way to \"read-in\" the big data into small data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Buffering (SQLite3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a single month's worth of data is very big, we're going to have to prune a lot of irrelevant data for the purposes of our application.\n",
    "\n",
    "### Feature Descriptions:\n",
    "\n",
    "Here are all the features given to for each data point (a comment in a subreddit:\n",
    "\n",
    "{Author, link_id, score, body, score_hidden (boolean), author_flair_text, gilded, subreddit, edited (boolean), author_flair_css_class, retrieved_on, name, created_utc, parent_id, controversiality, ups, distinguished, id, subreddit_id, downs, archived (boolean).}\n",
    "\n",
    "(vague in what some of these do)\n",
    "\n",
    "We're going to use the body, comment_id, and parent_id as the response to the closest match to a user's statement. Further, we'll use votes and gilded as a way of filtering \"irrelevant\" (in some vague sense) comments.\n",
    "\n",
    "In the future, we'll also be interested in filtering responses via sub-reddits.\n",
    "\n",
    "### The\n",
    "We're first need a database that stores a duple (comment, reply). The reason why is because these files are too big for us to just like read into RAM and then create the training files from a month basis (Reddit has many users). Thus,\n",
    "\n",
    "But chances are you're gonna want to eventually if you wanted to create a really\n",
    "\n",
    "Nice chat bot you're gonna be wanting to work on many months of data\n",
    "\n",
    "so maybe possibly billions of comments you do have that your disposal so when that's the case we\n",
    "\n",
    "Probably want to have some sort of database now for the purposes here. Just to keep things simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using sqlite3 for our database, json to load in the lines from the datadump, and then datetime really just for logging. This wont be totally necessary.\n",
    "\n",
    "Data is stored as JSON data dumps, named by year and month (YYYY-MM). They are compressed in .bz2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#month of data we're analyzing\n",
    "timeframe = '2015-05'\n",
    "\n",
    "\n",
    "#We wish to supply a list of actions to commit all at once, because committing to prune a large dataset sequentially is more costly.\n",
    "\n",
    "sql_transaction = []\n",
    "\n",
    "\n",
    "\n",
    "#First, we need to establish a connection and cursor. This is true with both SQLite and MySQL.\n",
    "connection = sqlite3.connect('{}.db'.format(timeframe))\n",
    "c = connection.cursor()\n",
    "\n",
    "#Used for the main body of code.\n",
    "start_row = 0\n",
    "cleanup = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to make our table. With SQLite, the database is created with the connect if it doesn't already exist. SQL is a different language and it is common to use all-caps to denote SQL specific commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We wish to \"execute\" an SQL command to make \n",
    "def create_table():\n",
    "    c.execute(\"CREATE TABLE IF NOT EXISTS parent_reply(parent_id TEXT PRIMARY KEY, comment_id TEXT UNIQUE, parent TEXT, comment TEXT, subreddit TEXT, unix INT, score INT)\")\n",
    "\n",
    "#Function to tokenize newline, return into newlinechar and the double quotes into single quotes.\n",
    "def format_data(data):\n",
    "    data = data.replace('\\n',' newlinechar ').replace('\\r',' newlinechar ').replace('\"',\"'\")\n",
    "    return data\n",
    "\n",
    "#Find parent id for the parent data.\n",
    "def find_parent(pid):\n",
    "    try:\n",
    "        sql = \"SELECT comment FROM parent_reply WHERE comment_id = '{}' LIMIT 1\".format(pid)\n",
    "        c.execute(sql)\n",
    "        result = c.fetchone()\n",
    "        if result != None:\n",
    "            return result[0]\n",
    "        else: return False\n",
    "    except Exception as e:\n",
    "        #print(str(e))\n",
    "        return False\n",
    "\n",
    "#Find score from PID.\n",
    "def find_existing_score(pid):\n",
    "    try:\n",
    "        sql = \"SELECT score FROM parent_reply WHERE parent_id = '{}' LIMIT 1\".format(pid)\n",
    "        c.execute(sql)\n",
    "        result = c.fetchone()\n",
    "        if result != None:\n",
    "            return result[0]\n",
    "        else: return False\n",
    "    except Exception as e:\n",
    "        #print(str(e))\n",
    "        return False\n",
    "    \n",
    "#Next, many comments are either deleted or removed, but also some comments are very long, or very short. We want to make sure comments are of an acceptable length for training, and that the comment wasn't removed or deleted:\n",
    "def acceptable(data):\n",
    "    if len(data.split(' ')) > 1000 or len(data) < 1:\n",
    "        return False\n",
    "    elif len(data) > 32000:\n",
    "        return False\n",
    "    elif data == '[deleted]':\n",
    "        return False\n",
    "    elif data == '[removed]':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "#Now we define our inject function.\n",
    "def sql_insert_replace_comment(commentid,parentid,parent,comment,subreddit,time,score):\n",
    "    try:\n",
    "        sql = \"\"\"UPDATE parent_reply SET parent_id = ?, comment_id = ?, parent = ?, comment = ?, subreddit = ?, unix = ?, score = ? WHERE parent_id =?;\"\"\".format(parentid, commentid, parent, comment, subreddit, int(time), score, parentid)\n",
    "        transaction_bldr(sql)\n",
    "    except Exception as e:\n",
    "        print('s0 insertion',str(e))\n",
    "\"\"\"\n",
    "That covers a situation where a comment is already paired with a parent, but we also need to cover comments that don't have parents (but might be a parent to another comment!) \n",
    "and comments that do have parents and those parents don't already have a reply. We can further build out insertion block:\n",
    "\"\"\"\n",
    "\n",
    "def sql_insert_has_parent(commentid,parentid,parent,comment,subreddit,time,score):\n",
    "    try:\n",
    "        sql = \"\"\"INSERT INTO parent_reply (parent_id, comment_id, parent, comment, subreddit, unix, score) VALUES (\"{}\",\"{}\",\"{}\",\"{}\",\"{}\",{},{});\"\"\".format(parentid, commentid, parent, comment, subreddit, int(time), score)\n",
    "        transaction_bldr(sql)\n",
    "    except Exception as e:\n",
    "        print('s0 insertion',str(e))\n",
    "\n",
    "def sql_insert_no_parent(commentid,parentid,comment,subreddit,time,score):\n",
    "    try:\n",
    "        sql = \"\"\"INSERT INTO parent_reply (parent_id, comment_id, comment, subreddit, unix, score) VALUES (\"{}\",\"{}\",\"{}\",\"{}\",{},{});\"\"\".format(parentid, commentid, comment, subreddit, int(time), score)\n",
    "        transaction_bldr(sql)\n",
    "    except Exception as e:\n",
    "        print('s0 insertion',str(e))\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "Finally, the last part of our code that we need now is that we need to build the transaction_bldr function.\n",
    "This function is used to build up insertion statements and commit them in groups, rather than one-by-one. Doing it this way will be much much quicker:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def transaction_bldr(sql):\n",
    "    global sql_transaction\n",
    "    sql_transaction.append(sql)\n",
    "    if len(sql_transaction) > 1000:\n",
    "        c.execute('BEGIN TRANSACTION')\n",
    "        for s in sql_transaction:\n",
    "            try:\n",
    "                c.execute(s)\n",
    "            except:\n",
    "                pass\n",
    "        connection.commit()\n",
    "        sql_transaction = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/paperspace/reddit_comment_dumps/RC_2015-05'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a4b1bb7f60db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#Begin buffering data while keeping track of progress overtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#with open('J:/chatdata/reddit_data/{}/RC_{}'.format(timeframe.split('-')[0],timeframe), buffering=1000) as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilespace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m#print(row)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/paperspace/reddit_comment_dumps/RC_2015-05'"
     ]
    }
   ],
   "source": [
    "\n",
    "filespace = '/home/paperspace/reddit_comment_dumps/RC_{}'\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    #Initiate empty table.\n",
    "    create_table()\n",
    "    #initialize counter for progress row_counter will tell just how far we're iterating\n",
    "    #paired_rows will tell us how many comment+reply we've paired.\n",
    "    row_counter = 0\n",
    "    paired_rows = 0\n",
    "\n",
    "    #Begin buffering data while keeping track of progress overtime\n",
    "    #with open('J:/chatdata/reddit_data/{}/RC_{}'.format(timeframe.split('-')[0],timeframe), buffering=1000) as f:\n",
    "    with open(filespace.format(timeframe), buffering=1000) as f:\n",
    "        for row in f:\n",
    "            #print(row)\n",
    "            #time.sleep(555)\n",
    "            row_counter += 1\n",
    "\n",
    "            #Next we wish to \"read-in\" the rows, which is in json format\n",
    "            if row_counter > start_row:\n",
    "                try:\n",
    "                    row = json.loads(row)\n",
    "                    parent_id = row['parent_id'].split('_')[1]\n",
    "                    body = format_data(row['body'])\n",
    "                    created_utc = row['created_utc']\n",
    "                    score = row['score']\n",
    "                    \n",
    "                    comment_id = row['id']\n",
    "                    \n",
    "                    subreddit = row['subreddit']\n",
    "                    parent_data = find_parent(parent_id)\n",
    "                    \n",
    "                    existing_comment_score = find_existing_score(parent_id)\n",
    "                    if existing_comment_score:\n",
    "                        if score > existing_comment_score:\n",
    "                            if acceptable(body):\n",
    "                                sql_insert_replace_comment(comment_id,parent_id,parent_data,body,subreddit,created_utc,score)\n",
    "                                \n",
    "                    else:\n",
    "                        if acceptable(body):\n",
    "                            if parent_data:\n",
    "                                if score >= 2:\n",
    "                                    sql_insert_has_parent(comment_id,parent_id,parent_data,body,subreddit,created_utc,score)\n",
    "                                    paired_rows += 1\n",
    "                            else:\n",
    "                                sql_insert_no_parent(comment_id,parent_id,body,subreddit,created_utc,score)\n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                            \n",
    "            if row_counter % 100000 == 0:\n",
    "                print('Total Rows Read: {}, Paired Rows: {}, Time: {}'.format(row_counter, paired_rows, str(datetime.now())))\n",
    "            \"\"\"\n",
    "            If you're training much larger datasets, you may find there is significant bloat that we need to handle for. \n",
    "            This is because only about 10% of the comments are getting paired, \n",
    "            so a large % of our database is not actually going to be used. I use the following additional code:\n",
    "            Directly below the other counter. This requires a new cleanup variable, \n",
    "            which specifies how many rows before you \"cleanup.\" \n",
    "            \n",
    "            This will remove bloat to our database and keep insertion speeds fairly high. \n",
    "            Each \"cleanup\" seems to cost about 2K pairs, pretty much wherever you put it. \n",
    "            If it's every 100K rows, that'll cost you 2K pairs per 100K rows. I went with 1 million. \n",
    "            Another option you have is to clean every 1 million rows, but clean not the last 1 million, \n",
    "            but instead the last -1,100,000 to the -100,000th row, since it seems those 2K pairs are happening in the last 100K. \n",
    "            Even with this though, you will still lose some pairs. \n",
    "            I felt like 2K pairs, out of 100K pairs per 1 million rows was negligible and not important. \n",
    "            I also added a start_row variable, so I could start and stop database inserting while trying to improve the speeds \n",
    "            a bit. The c.execute(\"VACUUM\") is an SQL command to shrink the size of the database down to what it ought to me. \n",
    "            This actually probably isn't required, and you might want to only do this at the very end. \n",
    "            I didn't test how long this operation takes. \n",
    "            I mostly just did it so I could see immediately after a delete what the size of the database was. \n",
    "            \"\"\"\n",
    "            if row_counter > start_row:\n",
    "                if row_counter % cleanup == 0:\n",
    "                    print(\"Cleanin up!\")\n",
    "                    sql = \"DELETE FROM parent_reply WHERE parent IS NULL\"\n",
    "                    c.execute(sql)\n",
    "                    connection.commit()\n",
    "                    c.execute(\"VACUUM\")\n",
    "                    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
